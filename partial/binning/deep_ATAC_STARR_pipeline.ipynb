{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pybedtools\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "from biodata.bed import BED3Reader\n",
    "from genomictools import GenomicCollection\n",
    "from genomictools import GenomicPos\n",
    "from collections import deque\n",
    "from scipy import stats\n",
    "from biodata.delimited import DelimitedReader, DelimitedWriter\n",
    "from biodata.bed import BEDGraphReader\n",
    "import builtins\n",
    "# from biodata.gff import GTFReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data & combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "def generate_grace_period_bins(file, gSize):\n",
    "  \"\"\"\n",
    "  Helper function for grace_period_bins.\n",
    "  \"\"\"\n",
    "#   print(len(file))\n",
    "  new_rows = []\n",
    "  for _, row in file.iterrows():\n",
    "    for i_sta in range(0,gSize+1):\n",
    "      for i_end in range(0, gSize+1):\n",
    "        # new_row = {0: row[0], 1: row[1]+i_sta, 2: row[2]+i_end, 3: row[3], 4: row[4], 5: row[5], 6: row[6], 7: row[7]}\n",
    "        # new_rows.append(new_row)\n",
    "        \n",
    "        # new_row = {0: row[0], 1: row[1]-i_sta, 2: row[2]+i_end, 3: row[3], 4: row[4], 5: row[5], 6: row[6], 7: row[7]}\n",
    "        # new_rows.append(new_row)\n",
    "\n",
    "        # new_row = {0: row[0], 1: row[1]+i_sta, 2: row[2]-i_end, 3: row[3], 4: row[4], 5: row[5], 6: row[6], 7: row[7]}\n",
    "        # new_rows.append(new_row)\n",
    "      \n",
    "        # new_row = {0: row[0], 1: row[1]-i_sta, 2: row[2]-i_end, 3: row[3], 4: row[4], 5: row[5], 6: row[6], 7: row[7]}\n",
    "        # new_rows.append(new_row)\n",
    "        new_row = {0: row[0], 1: row[1]+i_sta, 2: row[2]+i_end, 3: row[3]} # for EnhancerNet\n",
    "        new_rows.append(new_row)\n",
    "        \n",
    "        new_row = {0: row[0], 1: row[1]-i_sta, 2: row[2]+i_end, 3: row[3]}\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "        new_row = {0: row[0], 1: row[1]+i_sta, 2: row[2]-i_end, 3: row[3]}\n",
    "        new_rows.append(new_row)\n",
    "      \n",
    "        new_row = {0: row[0], 1: row[1]-i_sta, 2: row[2]-i_end, 3: row[3]}\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "  file = pd.concat([file, pd.DataFrame(new_rows)], ignore_index=True).drop_duplicates()\n",
    "#   print(len(file))\n",
    "  print(\"finished generating reference\")\n",
    "  return file\n",
    "\n",
    "\n",
    "def grace_period_bins(bedfile,gSize):\n",
    "  \"\"\"\n",
    "  Generate a sorted dataframe of binned fragments for respected bedfiles.\n",
    "  Modified: Use 5bp grace period bins for all references. This means that the regions \n",
    "  with at most 5bp differences with the original regions are all counted as \n",
    "  possible bins.\n",
    "  \"\"\"\n",
    "  file = pybedtools.BedTool(bedfile).to_dataframe(disable_auto_names=True, header=None)\n",
    "  return generate_grace_period_bins(file, gSize).sort_values(by=[0, 1, 2])\n",
    "\n",
    "\n",
    "def get_reference(path, type, idx):\n",
    "    \"\"\" \n",
    "    Return the orientation-separated counts file in a dataframe for respective replicate.\n",
    "    \"\"\"\n",
    "    df2 = pybedtools.BedTool(path+type+idx+\"/all/count.bed.gz\").to_dataframe(disable_auto_names=True, header=None)\n",
    "    f = df2[df2[3] == \"+\"]\n",
    "    r = df2[df2[3] == \"-\"]\n",
    "    return f, r\n",
    "\n",
    "\n",
    "### func for align reference with diff grace period bins\n",
    "def align(ls):\n",
    "  \"\"\"\n",
    "  Align the sorted STARR-seq segments with different grace period bins.\n",
    "  Return a list that contains 100% sequence coverage file:\n",
    "  (chr, start, end, counts)\n",
    "  \"\"\"\n",
    "  input = pybedtools.BedTool.from_dataframe(ls[0]) # forward/reverse counts.bed.gz\n",
    "  file = pybedtools.BedTool.from_dataframe(ls[1]) # binned reference\n",
    "  # print(file)\n",
    "  overlap = file.coverage(input, sorted=True, counts=True, f=1.0, r=True, n=48)\n",
    "  overlap = overlap.to_dataframe(disable_auto_names=True, header=None)\n",
    "  return overlap\n",
    "\n",
    "\n",
    "def count_mapped_bins(UMI, counts, ori_ref, ref, data_type, idx, design, orientation):\n",
    "# def count_mapped_bins(UMI, counts, ori_ref, ref): # for test\n",
    "  \"\"\" \n",
    "  Need to go to the original orientation separated counts.bed.gz file \n",
    "  to sum up the overlapped bins read counts if necessary (RNA).\n",
    "  Sum the reads from fragmented bins.\n",
    "\n",
    "  Parameter UMI: type boolean; True for RNA, False for DNA\n",
    "  Parameter counts: dataframe (aligned fragmented bins counts) from pybedtools.coverage\n",
    "  Parameter ori_ref: dataframe (elements regions after design)\n",
    "  Parameter ref: forward/reverse counts.bed.gz file\n",
    "  Parameter data_type: string, DNA or RNA\n",
    "  Parameter idx: string, 1-6 for DNA, 1-3 for RNA\n",
    "  Parameter design: string, e.g. TSS_b\n",
    "  Parameter orientation: string, f or r (forward or reverse)\n",
    "  \"\"\"\n",
    "  if UMI: # RNA samples\n",
    "    ## subset only overlapped elements from pybedtools.coverage\n",
    "    # counts = counts[counts[8]==1].reset_index(drop=True)\n",
    "    counts = counts[counts[4]==1].reset_index(drop=True)\n",
    "    ## replace the reads since the dataset has already been deduplicated with UMI\n",
    "    ## also merge the fragmented bins through element id\n",
    "    # counts[8] = ref.merge(counts, on=[0,1,2], how=\"inner\").loc[:,[\"4_x\"]]\n",
    "    counts[\"4_y\"] = ref.merge(counts, on=[0,1,2], how=\"inner\").loc[:,[\"4_x\"]]\n",
    "    # counts = counts.loc[:,[0,1,2,3,8]]\n",
    "    counts = counts.loc[:,[0,1,2,3,\"4_y\"]]\n",
    "    counts = counts.groupby(3, as_index=True).agg({0: 'first', 1: 'first', 2: 'first', \"4_y\": 'sum'})\n",
    "\n",
    "  else: # DNA samples\n",
    "    ## subset only overlapped elements from pybedtools.coverage\n",
    "    # counts = counts.merge(ref, on=[0,1,2], how=\"inner\").loc[:,[0,1,2,\"3_x\",8]]\n",
    "    counts = counts.merge(ref, on=[0,1,2], how=\"inner\").loc[:,[0,1,2,\"3_x\",\"4_y\"]] #for EnhancerNet\n",
    "    \n",
    "    ## merge the fragmented bins through element id\n",
    "    # counts = counts.groupby(\"3_x\", as_index=True).agg({0: 'first', 1: 'first', 2: 'first', 8: 'sum'})\n",
    "    counts = counts.groupby(\"3_x\", as_index=True).agg({0: 'first', 1: 'first', 2: 'first', \"4_y\": 'sum'})\n",
    "\n",
    "  ## replace the values of fragmented bins by original values retrieved from element id\n",
    "  if not counts.empty:\n",
    "    merged_df = counts.merge(ori_ref, left_index=True, right_on=3, how='left')\n",
    "    \n",
    "    # output = merged_df.loc[:, [\"0_y\",\"1_y\", \"2_y\", 8]]\n",
    "    output = merged_df.loc[:, [\"0_y\",\"1_y\", \"2_y\", \"4_y\"]]\n",
    "    print(output)\n",
    "  else:\n",
    "    output = pd.DataFrame()\n",
    "  \n",
    "  # counts.to_csv(\"test_output_n.bed\", sep=\"\\t\", index=False, header=False)\n",
    "  # output.to_csv(\"/fs/cbsuhy01/storage/yz2676/data/STARR-seq/partial/data/deep_ATAC_STARR/DNA/\"+data_type+idx+\"_\"+orientation+\"_\"+design+\".bed\", sep='\\t', index=False, header=False)\n",
    "  output.to_csv(\"/fs/cbsuhy01/storage/yz2676/data/STARR-seq/enhancer_end/data/deep_ATAC_STARR/\"+data_type+idx+\"_\"+orientation+\"_\"+design+\".bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import DNA/RNA sequence\n",
    "## path for DNA files\n",
    "# path = \"/fs/cbsuhy01/storage/jz855/STARR_seq_dataset/deep_ATAC_STARR/processing_data_v1/out_DNA_no_UMI/\"\n",
    "# path = \"/fs/cbsuhy01/storage/jz855/STARR_seq_dataset/WHG_STARR_seq_TR/processing_data_v1/out_DNA_no_UMI/\"\n",
    "## path for RNA files\n",
    "path = \"/fs/cbsuhy01/storage/jz855/STARR_seq_dataset/deep_ATAC_STARR/processing_data_v1/out_RNA_with_UMI/\"\n",
    "# path = \"/fs/cbsuhy01/storage/jz855/STARR_seq_dataset/WHG_STARR_seq_TR/processing_data_v1/out_RNA_no_UMI/\"\n",
    "\n",
    "data_type = \"RNA\"\n",
    "idx = \"3\"\n",
    "forward, reverse = get_reference(path, data_type, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for partial elements\n",
    "from multiprocessing import Pool\n",
    "UMI = True #False for DNA; True for RNA\n",
    "\n",
    "#1\n",
    "ref_path = \"../../output/divergent_60bp_without_TSS_b.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"TSS_b\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"TSS_b\", \"r\")\n",
    "\n",
    "#2\n",
    "ref_path = \"../../output/divergent_60bp_without_TSS_p.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"TSS_p\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"TSS_p\", \"r\")\n",
    "\n",
    "#3\n",
    "ref_path = \"../../output/divergent_60bp_without_TSS_n.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"TSS_n\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"TSS_n\", \"r\")\n",
    "\n",
    "#4\n",
    "ref_path = \"../../output/divergent_60bp_without_pause_site_b.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"pause_site_b\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"pause_site_b\", \"r\")\n",
    "\n",
    "#5\n",
    "ref_path = \"../../output/divergent_60bp_without_pause_site_p.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"pause_site_p\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"pause_site_p\", \"r\")\n",
    "\n",
    "#6\n",
    "ref_path = \"../../output/divergent_60bp_without_pause_site_n.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"pause_site_n\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"pause_site_n\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating reference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0_y       1_y       2_y  4_y\n",
      "0  chr17  21043339  21043593    1\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "UMI = True #False for DNA; True for RNA\n",
    "\n",
    "#1\n",
    "ref_path = \"../../enhancer_end/Enhancer_3p.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"3p\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"3p\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deep_ATAC_STARR\n",
    "# orientation = \"_r_\"\n",
    "# design_type = \"pause_site_b\"\n",
    "# DNA_path = [\"../data/deep_ATAC_STARR/DNA/DNA1\"+orientation+design_type+\".bed\",\"../data/deep_ATAC_STARR/DNA/DNA2\"+orientation+design_type+\".bed\",\\\n",
    "#             \"../data/deep_ATAC_STARR/DNA/DNA3\"+orientation+design_type+\".bed\",\"../data/deep_ATAC_STARR/DNA/DNA4\"+orientation+design_type+\".bed\",\\\n",
    "#             \"../data/deep_ATAC_STARR/DNA/DNA5\"+orientation+design_type+\".bed\",\"../data/deep_ATAC_STARR/DNA/DNA6\"+orientation+design_type+\".bed\"]\n",
    "# RNA_path = [\"../data/deep_ATAC_STARR/RNA/RNA1\"+orientation+design_type+\".bed\",\"../data/deep_ATAC_STARR/RNA/RNA2\"+orientation+design_type+\".bed\",\\\n",
    "#             \"../data/deep_ATAC_STARR/RNA/RNA3\"+orientation+design_type+\".bed\",\"../data/deep_ATAC_STARR/RNA/RNA4\"+orientation+design_type+\".bed\"]\n",
    "\n",
    "## EnhancerNet\n",
    "orientation = \"_r_\"\n",
    "design_type = \"3p\"\n",
    "DNA_path = [\"../../enhancer_end/data/deep_ATAC_STARR/DNA1\"+orientation+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA2\"+orientation+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/DNA3\"+orientation+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA4\"+orientation+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/DNA5\"+orientation+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA6\"+orientation+design_type+\".bed\"]\n",
    "RNA_path = [\"../../enhancer_end/data/deep_ATAC_STARR/RNA1\"+orientation+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/RNA2\"+orientation+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/RNA3\"+orientation+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/RNA4\"+orientation+design_type+\".bed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### func for combining results from biological repeats + DNA/RNA\n",
    "def combine(input_list,condition):\n",
    "  \"\"\"\n",
    "  Combine the alignment files into a big matrix.\n",
    "  \"\"\"\n",
    "  output_1 = input_list[0].merge(input_list[1], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2], suffixes=('_1', '_2'))\n",
    "  output_2 = output_1.merge(input_list[2], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_3\"})\n",
    "  output_3 = output_2.merge(input_list[3], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_4\"})\n",
    "  output_4 = output_3.merge(input_list[4], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_5\"})\n",
    "  output_5 = output_4.merge(input_list[5], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_6\"})\n",
    "  # print(output_5)\n",
    "  output_6 = output_5.merge(input_list[6], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_7\"})\n",
    "  output_7 = output_6.merge(input_list[7], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_8\"})\n",
    "  output_8 = output_7.merge(input_list[8], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_9\"})\n",
    "  output_9 = output_8.merge(input_list[9], how=\"outer\",left_on=[0,1,2],right_on=[0,1,2]).rename(columns={3:\"3_10\"})\n",
    "  output_9 = output_9.fillna(0)\n",
    "\n",
    "  output_9 = output_9[output_9[0] != \"chrN\"]\n",
    "  ## change the next two lines when combining full elements\n",
    "  # output_9.to_csv(\"../data/deep_ATAC_STARR/\"+condition[1]+\".bed\", sep='\\t', index=False, header=False)\n",
    "  output_9.to_csv(\"../../enhancer_end/data/\"+condition[1]+\".bed\", sep='\\t', index=False, header=False)\n",
    "  # cmds = [\"sort -k1,1 -V ../data/deep_ATAC_STARR/\"+condition[1]+\".bed > ../data/deep_ATAC_STARR/srt\"+condition[1]+condition[0]+\".bed\",\\\n",
    "  #          \"rm ../data/deep_ATAC_STARR/\"+condition[1]+\".bed\"]\n",
    "  cmds = [\"sort -k1,1 -V ../../enhancer_end/data/\"+condition[1]+\".bed > ../../enhancer_end/data/srt\"+condition[1]+condition[0]+\".bed\",\\\n",
    "           \"rm ../../enhancer_end/data/\"+condition[1]+\".bed\"]\n",
    "  \n",
    "  # output_9.to_csv(\"../../../SOLARR/activity/data/\"+condition+\".bed\", sep='\\t', index=False, header=False)\n",
    "  # cmds = [\"sort -k1,1 -V ../../../SOLARR/activity/data/\"+condition+\".bed > ../../../SOLARR/activity/data/srt_\"+condition+\".bed\",\\\n",
    "  #          \"rm ../../../SOLARR/activity/data/\"+condition+\".bed\"]\n",
    "  for cmd in cmds:\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "ls = DNA_path + RNA_path\n",
    "input_ls = [] \n",
    "for index in range(len(ls)):\n",
    "    # print(index)\n",
    "    file = pybedtools.BedTool(ls[index])\n",
    "    if file == \"\": # deal with completely nan file\n",
    "      print(\"here\")\n",
    "      # need to append the coordinates and zeros as placeholders, or it cannot merge in the next step??\n",
    "      ph_data = {0:[\"chrN\"],1:[1.0],2:[2.0],3:[0.0]}\n",
    "      df = pd.DataFrame(ph_data)\n",
    "      # print(ph_data)\n",
    "      input_ls.append(df) \n",
    "    else:\n",
    "      output_file = file.to_dataframe(disable_auto_names=True, header=None)\n",
    "      input_ls.append(output_file)\n",
    "\n",
    "combine(input_ls,[design_type,orientation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "refer[3] = [\"Divergent\"+str(i) for i in range(len(refer))]\n",
    "print(refer) \n",
    "refer.to_csv(ref_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating reference\n",
      "        0_y        1_y        2_y  4_y\n",
      "1047  chr10   73772486   73772842    4\n",
      "1052  chr10   74825758   74826052    1\n",
      "1064  chr10   79826479   79826817    1\n",
      "1088  chr10   92830995   92831262    2\n",
      "1126  chr10  101060529  101060938    4\n",
      "...     ...        ...        ...  ...\n",
      "872    chr1  246724211  246724606    7\n",
      "911   chr10   14959124   14959504    2\n",
      "921   chr10   21525992   21526279    4\n",
      "966   chr10   46217349   46217586    1\n",
      "989   chr10   59906776   59907081    2\n",
      "\n",
      "[207 rows x 4 columns]\n",
      "        0_y        1_y        2_y  4_y\n",
      "1057  chr10   75235610   75235963    3\n",
      "1071  chr10   86959078   86959527    2\n",
      "1165  chr10  110672363  110672654    2\n",
      "1180  chr10  120851091  120851505    1\n",
      "1186  chr10  123135035  123135490    2\n",
      "...     ...        ...        ...  ...\n",
      "893   chr10    5479482    5479955    1\n",
      "920   chr10   21525835   21526185    1\n",
      "932   chr10   24921352   24921652    2\n",
      "969   chr10   47461283   47461625    1\n",
      "982   chr10   50623827   50624206    1\n",
      "\n",
      "[211 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# for full elements\n",
    "from multiprocessing import Pool\n",
    "UMI = True #False for DNA; True for RNA\n",
    "\n",
    "# ref_path = \"../../GRO_cap_PINTS_qpeak_call_117/K562_GROcap_hg38_1.1.7_qpeak_calls_1_divergent_peaks_element_60bp_e.bed\"\n",
    "ref_path = \"../../enhancer_end/Enhancer_K562_5p+60_boundaries_pause_site_b.bed\"\n",
    "reference = grace_period_bins(ref_path, 5)\n",
    "with Pool(10) as p:\n",
    "  a_forward, a_reverse = p.map(align, [[forward, reference], [reverse, reference]])\n",
    "\n",
    "ori_ref = pybedtools.BedTool(ref_path).to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "count_mapped_bins(UMI, a_forward, ori_ref, forward, data_type, idx, \"full\", \"f\")\n",
    "count_mapped_bins(UMI, a_reverse, ori_ref, reverse, data_type, idx, \"full\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deep_ATAC_STARR\n",
    "design_type = \"r_full\"\n",
    "# DNA_path = [\"../../full/DNA1_\"+design_type+\".bed\",\"../../full/DNA2_\"+design_type+\".bed\",\\\n",
    "#             \"../../full/DNA3_\"+design_type+\".bed\",\"../../full/DNA4_\"+design_type+\".bed\",\\\n",
    "#             \"../../full/DNA5_\"+design_type+\".bed\",\"../../full/DNA6_\"+design_type+\".bed\"]\n",
    "# RNA_path = [\"../../full/RNA1_\"+design_type+\".bed\",\"../../full/RNA2_\"+design_type+\".bed\",\\\n",
    "#             \"../../full/RNA3_\"+design_type+\".bed\",\"../../full/RNA4_\"+design_type+\".bed\"]\n",
    "DNA_path = [\"../../enhancer_end/data/deep_ATAC_STARR/DNA1_\"+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA2_\"+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/DNA3_\"+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA4_\"+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/DNA5_\"+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/DNA6_\"+design_type+\".bed\"]\n",
    "RNA_path = [\"../../enhancer_end/data/deep_ATAC_STARR/RNA1_\"+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/RNA2_\"+design_type+\".bed\",\\\n",
    "            \"../../enhancer_end/data/deep_ATAC_STARR/RNA3_\"+design_type+\".bed\",\"../../enhancer_end/data/deep_ATAC_STARR/RNA4_\"+design_type+\".bed\"]\n",
    "\n",
    "ls = DNA_path + RNA_path\n",
    "input_ls = []\n",
    "for index in range(len(ls)):\n",
    "    file = pybedtools.BedTool(ls[index])\n",
    "    output_file = file.to_dataframe(disable_auto_names=True, header=None)\n",
    "    input_ls.append(output_file)\n",
    "combine(input_ls, design_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation-independent enh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for checking how many full elements have both forward and reverse strand\n",
    "### then go to rstudio to check how many of them are orientation-independently functioned\n",
    "# forward = pybedtools.BedTool(\"../../full/srt_f_full.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "forward = pybedtools.BedTool(\"../../enhancer_end/data/srt_f_full.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "reverse = pybedtools.BedTool(\"../../enhancer_end/data/srt_r_full.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "overlap = pd.merge(forward, reverse, how ='inner', on = [0, 1, 2])\n",
    "fd = overlap.iloc[:,0:13]\n",
    "rv = overlap.iloc[:,list(range(0, 3)) + list(range(13, 23))]\n",
    "# print(rv)\n",
    "# print(\"full: \", len(overlap))\n",
    "fd.to_csv(\"../../enhancer_end/data/srt_f_full.bed\", sep='\\t', index=False, header=False)\n",
    "rv.to_csv(\"../../enhancer_end/data/srt_r_full.bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### overlap full elements with deletion files\n",
    "## first: merge f/r files for full elements and save it\n",
    "forward = pybedtools.BedTool(\"../../enhancer_end/data/srt_f_full_e.bed\")\n",
    "forward = forward.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "reverse = pybedtools.BedTool(\"../../enhancer_end/data//srt_r_full_e.bed\")\n",
    "reverse = reverse.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "# defined as enhancers if exhibit enhancer activity on both sides\n",
    "both = pd.merge(forward, reverse, how ='inner', on = [0, 1, 2]).iloc[:,0:3]\n",
    "\n",
    "forward = pd.merge(forward, both, how ='inner', on = [0, 1, 2])\n",
    "reverse = pd.merge(reverse, both, how ='inner', on = [0, 1, 2])\n",
    "\n",
    "overlap = pd.concat([forward,reverse],ignore_index=True).groupby([0,1,2]).sum().reset_index()\n",
    "print(overlap)\n",
    "overlap.to_csv(\"../../enhancer_end/data/srt_full_ori_indep_e.bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0        3_y        4_y        name\n",
      "0  chr17   21043339   21043593  Divergent0\n",
      "1   chr5   43064745   43065040  Divergent1\n",
      "2   chr7  100867119  100867498  Divergent2\n"
     ]
    }
   ],
   "source": [
    "### save the 3p elements\n",
    "full = pybedtools.BedTool(\"../../enhancer_end/data/srt_full_ori_indep_e.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "ori = pybedtools.BedTool(\"../../enhancer_end/3p_boundaries.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "ori[3]= ori[3]-60\n",
    "ori[4] = ori[4]+60\n",
    "test = pd.DataFrame()\n",
    "test[1] = ori[3]\n",
    "test[2] = ori[4]\n",
    "test[3] = ori[1]\n",
    "test[4] = ori[2]\n",
    "ori[1] = test[1]\n",
    "ori[2] = test[2]\n",
    "ori[3] = test[3]\n",
    "ori[4] = test[4]\n",
    "\n",
    "test = full.merge(ori, how=\"inner\", on=[0,1,2]).iloc[:,[0,-2,-1]]\n",
    "test[\"name\"] = [\"Divergent\"+str(i) for i in range(len(test))]\n",
    "print(test)\n",
    "test.to_csv(\"../../enhancer_end/Enhancer_3p.bed\", sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged full elements files for TSS, pause_site and DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fifth: get the merged full file for pause_site and dpr\n",
    "full_tss_p = pybedtools.BedTool(\"../../full/data/filtered/full_TSS_p.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_tss_n = pybedtools.BedTool(\"../../full/data/filtered/full_TSS_n.bed\").to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_tss_pn = pd.merge(full_tss_p, full_tss_n, how ='outer', on = [0, 1, 2])\n",
    "# print(full_tss_pn)\n",
    "output = full_tss_pn.iloc[:,0:13] # for full elements, it doesn't matter whether it's on f/r strand\n",
    "output.iloc[-3:,:] = full_tss_pn.iloc[23:, list(range(0, 3)) + list(range(13, 23))].values\n",
    "print(output)\n",
    "\n",
    "output.to_csv(\"../../full/data/filtered/full_tss_pn.bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fifth: get the merged full file for pause_site and dpr\n",
    "full_ps_p = pybedtools.BedTool(\"../../full/data/filtered/full_pause_site_p.bed\")\n",
    "full_ps_p = full_ps_p.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_ps_n = pybedtools.BedTool(\"../../full/data/filtered/full_pause_site_n.bed\")\n",
    "full_ps_n = full_ps_n.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_ps_pn = pd.merge(full_ps_p, full_ps_n, how ='outer', on = [0, 1, 2])\n",
    "# print(full_ps_pn)\n",
    "output = full_ps_pn.iloc[:,0:13] # for full elements, it doesn't matter whether it's on f/r strand\n",
    "output.iloc[-2:,:] = full_ps_pn.iloc[32:, list(range(0, 3)) + list(range(13, 23))].values\n",
    "print(output)\n",
    "\n",
    "output.to_csv(\"../../full/data/filtered/full_ps_pn.bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same pipeline for dpr\n",
    "typ = \"low\"\n",
    "full_dpr_p = pybedtools.BedTool(\"../../full/data/filtered/srt_full_DPR_\"+typ+\"_conf_p.bed\")\n",
    "full_dpr_p = full_dpr_p.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_dpr_n = pybedtools.BedTool(\"../../full/data/filtered/srt_full_DPR_\"+typ+\"_conf_n.bed\")\n",
    "full_dpr_n = full_dpr_n.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full_dpr_pn = pd.merge(full_dpr_p, full_dpr_n, how ='outer', on = [0, 1, 2])\n",
    "# print(full_dpr_pn)\n",
    "output = full_dpr_pn.iloc[:,0:13]\n",
    "output.iloc[-3:] = full_dpr_pn.iloc[5:, list(range(0, 3)) + list(range(13, 23))].values\n",
    "print(output)\n",
    "\n",
    "output.to_csv(\"../../full/data/filtered/full_dpr_\"+typ+\"_pn.bed\", sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify orientation by Maximum/minimum TSS\n",
    "1. First we will check GRO-cap read counts for full elements for corresponding files (reverse the binned elements to original elements), and then save intermediate to_exchange and no_exchange dictionaries for each file.  \n",
    "For instance, for forward pause site file, it could have elements that are in the reverse dictionary and have less reads that that in the reverse file, and these are saved in the to_exchange dictionary. Otherwise, when it has elements that are not in the reverse file but have less reads than that in the reverse file, we save them inside the no_exchange dictionary.  \n",
    "For sanity check, the number of to_exchange dictionary should be of the same length for f_to_exchange and r_to_exchange.  \n",
    "\n",
    "2. Secondly, load in the files for exchange. First we will use intermediate dics to save the elements and exchange those entries in the to_exchange dic. Then we will add in corresponding no_exchange entries and sort them accordingly.  \n",
    "\n",
    "3. Finally we merge on exch_f, exch_r files, and fill in with NAN values.  \n",
    "\n",
    "4. Then we will apply the same approach on elements with no design edits (no deletion).  \n",
    "\n",
    "5. For both sides deletion files, we merge them with preexisting full element files, if the size was no bigger than previous ones, then the previous file that contains the union of _f, _r full elements can be used directly; if not, then we need to generate new files for both full and partial designs. After creating the lastest full element file, we merge on _b and full files, and fill in with NAN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### should have the original full element boundaries before going to the next step:\n",
    "### if not, run this cell\n",
    "from reverse_original import revert\n",
    "file_input = \"../../full/data/PROcap_clear/filtered/srt_full_DPR_p.bed\"\n",
    "revert(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort if necessary - pipeline need re-arrange!!\n",
    "des = \"TSS_n\"\n",
    "cmds = [\"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/\"+des+\".bed > ../data/deep_ATAC_STARR/filtered/srt_\"+des+\".bed\",\\\n",
    "           \"rm ../data/deep_ATAC_STARR/filtered/\"+des+\".bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = \"full_TSS_b\"\n",
    "cmds = [\"sort -k1,1 -V ../../full/data/filtered/\"+des+\".bed > ../../full/data/filtered/srt_\"+des+\".bed\",\\\n",
    "           \"rm ../../full/data/filtered/\"+des+\".bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### third: check total GRO-cap read counts from p/n files, see if they need to be exchanged\n",
    "### this applies not only to TSS analysis, bc we are calculating the maximum/minimum TSS concept\n",
    "## some divergent elements are not that convincing, might need to delete them\n",
    "bw1 = pyBigWig.open(\"../../../GROcap/K562_GROcap_hg38_aligned_pl.bw\")\n",
    "bw2 = pyBigWig.open(\"../../../GROcap/K562_GROcap_hg38_aligned_mn.bw\")\n",
    "# bw1 = pyBigWig.open(\"../../20231024_PROcap/alignmentmerge_C1a_and_C1b_pl.bw\") # for PROcap\n",
    "# bw2 = pyBigWig.open(\"../../20231024_PROcap/alignmentmerge_C1a_and_C1b_mn.bw\")\n",
    "\n",
    "# binned full elements need to find their corresponding original boundaries\n",
    "# check both positive deletion and negative\n",
    "des = \"TSS\"\n",
    "fp = open(\"../../full/data/filtered/srt_full_\"+des+\"_p.bed\", \"r\") # change to TSS/ps accordingly\n",
    "fr = open(\"../../full/data/filtered/srt_full_\"+des+\"_n.bed\", \"r\")\n",
    "# fp = open(\"../../full/data/PROcap_clear/filtered/original_full_dpr_p.bed\", \"r\") # change to TSS/ps/dpr accordingly\n",
    "# fr = open(\"../../full/data/PROcap_clear/filtered/original_full_dpr_n.bed\", \"r\")\n",
    "line = fp.readline()\n",
    "fp_ls = []\n",
    "while line:\n",
    "    ls = line.strip(\"\\n\").split(\"\\t\")\n",
    "    fp_ls.append(ls)\n",
    "    line = fp.readline()\n",
    "\n",
    "fn_ls = []\n",
    "line1 = fr.readline()\n",
    "while line1:\n",
    "    ls = line1.strip(\"\\n\").split(\"\\t\")\n",
    "    fn_ls.append(ls)\n",
    "    line1 = fr.readline()\n",
    "\n",
    "# need to count in whether they have corresponding negative strand\n",
    "# save the paired elements into a dictionary, can use index to extract\n",
    "count = 0\n",
    "f_dic_to_exchange = {}\n",
    "f_dic_no_exchange = {}\n",
    "while count < len(fp_ls):\n",
    "    p = fp_ls[count]\n",
    "    chr = p[0]\n",
    "    start = int(p[1])\n",
    "    end = int(p[2])\n",
    "    if np.mean(np.nan_to_num(bw1.values(chr, start, end))) < abs(np.mean(np.nan_to_num(bw2.values(chr, start, end)))):\n",
    "        if p in fn_ls:\n",
    "            f_dic_to_exchange[count] = p\n",
    "        else:\n",
    "            f_dic_no_exchange[count] = p\n",
    "    count += 1\n",
    "\n",
    "count = 0\n",
    "r_dic_to_exchange = {}\n",
    "r_dic_no_exchange = {}\n",
    "while count < len(fn_ls):\n",
    "    n = fn_ls[count]\n",
    "    chr = n[0]\n",
    "    start = int(n[1])\n",
    "    end = int(n[2])\n",
    "    if np.mean(np.nan_to_num(bw1.values(chr, start, end))) < abs(np.mean(np.nan_to_num(bw2.values(chr, start, end)))):\n",
    "        if n in fp_ls:\n",
    "            r_dic_to_exchange[count] = n\n",
    "        else:\n",
    "            r_dic_no_exchange[count] = n\n",
    "    count += 1\n",
    "\n",
    "print(f_dic_to_exchange.keys())\n",
    "print(r_dic_to_exchange.keys())\n",
    "assert list(f_dic_to_exchange.values()) == list(r_dic_to_exchange.values()), \"To_exchange dic of different length\"\n",
    "print(len(f_dic_no_exchange), f_dic_no_exchange.keys())\n",
    "print(len(r_dic_no_exchange), r_dic_no_exchange.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### exchange the positive and negative strand data in to_exchange list\n",
    "### save the no_exchange to their corresponding f/r file\n",
    "### as printed in the previous cell, there are no elements in the negative strand that needs to save in the other file\n",
    "des = \"TSS\"\n",
    "fo = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_\"+des+\"_p.bed\") # change to TSS/pause_site/dpr\n",
    "fo = fo.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "re = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_\"+des+\"_n.bed\")\n",
    "re = re.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "# Create new DataFrames with rows exchanged\n",
    "exchanged_df1 = fo.copy()\n",
    "exchanged_df2 = re.copy()\n",
    "\n",
    "exchanged_df1.iloc[list(f_dic_to_exchange.keys())] = re.iloc[list(r_dic_to_exchange.keys())].values\n",
    "exchanged_df2.iloc[list(r_dic_to_exchange.keys())] = fo.iloc[list(f_dic_to_exchange.keys())].values\n",
    "\n",
    "# print(exchanged_df1)\n",
    "# Delete the rows from df1 and add them to df2 if in no_exchange dictionary\n",
    "exchanged_df1 = exchanged_df1[~fo.index.isin(list(f_dic_no_exchange.keys()))].dropna()\n",
    "exchanged_df2 = exchanged_df2[~re.index.isin(list(r_dic_no_exchange.keys()))].dropna()\n",
    "\n",
    "exchanged_df2 = pd.concat([exchanged_df2, fo.iloc[list(f_dic_no_exchange.keys())]])\n",
    "exchanged_df1 = pd.concat([exchanged_df1, re.iloc[list(r_dic_no_exchange.keys())]])\n",
    "print(len(exchanged_df1))\n",
    "print(len(exchanged_df2))\n",
    "\n",
    "# Save the elements\n",
    "exchanged_df1.to_csv(\"../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_p.bed\", sep='\\t', index=False, header=False)\n",
    "exchanged_df2.to_csv(\"../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_n.bed\", sep='\\t', index=False, header=False) # not sorted after adding to the end\n",
    "\n",
    "cmds = [\"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_n.bed > ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_min.bed\",\\\n",
    "           \"rm ../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_n.bed\",\\\n",
    "        \"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_p.bed > ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_max.bed\",\\\n",
    "           \"rm ../data/deep_ATAC_STARR/filtered/exch_\"+des+\"_p.bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge p/n, save NAN if they don't have the corresponding element\n",
    "exchanged_df1 = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_max.bed\")\n",
    "exchanged_df1 = exchanged_df1.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "exchanged_df2 = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_min.bed\")\n",
    "exchanged_df2 = exchanged_df2.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "output = pd.merge(exchanged_df1, exchanged_df2, how ='outer', on = [0, 1, 2])\n",
    "df1 = output.iloc[:,0:13]\n",
    "df2 = output.iloc[:,list(range(0, 3)) + list(range(13, 23))]\n",
    "print(len(df1))\n",
    "\n",
    "# Fill NAN values with 0 (for first try)\n",
    "df1 = df1.fillna(0)\n",
    "df2 = df2.fillna(0)\n",
    "\n",
    "df1.to_csv(\"../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_p.bed\", sep='\\t', index=False, header=False) # unsorted\n",
    "df2.to_csv(\"../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_n.bed\", sep='\\t', index=False, header=False)\n",
    "\n",
    "cmds = [\"rm ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_min.bed\",\\\n",
    "        \"rm ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_max.bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting\n",
    "cmds = [\"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_p.bed > ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_max.bed\",\\\n",
    "        \"rm ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_p.bed\",\\\n",
    "        \"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_n.bed > ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_min.bed\",\\\n",
    "        \"rm ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_n.bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For full elements, we will just load in previous merged files and sort them\n",
    "cmds = [\"sort -k1,1 -V ../../full/data/filtered/full_tss_pn.bed > ../../full/data/filtered/srt_full_tss.bed\",\\\n",
    "        \"rm ../../full/data/filtered/full_tss_pn.bed\"]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check if _b elements all contained in full\n",
    "des = \"TSS\"\n",
    "b = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_\"+des+\"_b.bed\")\n",
    "b = b.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "full = pybedtools.BedTool(\"../../full/data/filtered/srt_full_\"+des+\".bed\")\n",
    "full = full.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "output = pd.merge(b, full, how ='outer', on = [0, 1, 2])\n",
    "df1 = output.iloc[:,0:13]\n",
    "# print(df1)\n",
    "df1 = df1.fillna(0)\n",
    "df1.to_csv(\"../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_b.bed\", sep='\\t', index=False, header=False) # unsorted\n",
    "cmds = [\"sort -k1,1 -V ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_b.bed > ../data/deep_ATAC_STARR/filtered/srt_exch_\"+des+\"_b.bed\",\\\n",
    "        \"rm ../data/deep_ATAC_STARR/filtered/srt_t_exch_\"+des+\"_b.bed\",]\n",
    "for cmd in cmds:\n",
    "  os.system(cmd)\n",
    "\n",
    "if len(df1) != len(full): # new element in _b not in _n/_p\n",
    "  print(\"not equal\")\n",
    "  # f = pd.merge(full, df1, how ='outer', on = [0, 1, 2]).iloc[:,0:13].fillna(0)\n",
    "\n",
    "  # p = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_exch_ps_p.bed\")\n",
    "  # p = p.to_dataframe(disable_auto_names=True, header=None)\n",
    "  # n = pybedtools.BedTool(\"../data/deep_ATAC_STARR/filtered/srt_exch_ps_n.bed\")\n",
    "  # n = n.to_dataframe(disable_auto_names=True, header=None)\n",
    "\n",
    "  # df2 = pd.merge(p, df1, how ='outer', on = [0, 1, 2]).iloc[:,0:13].fillna(0)\n",
    "  # df3 = pd.merge(n, df1, how ='outer', on = [0, 1, 2]).iloc[:,0:13].fillna(0)\n",
    "\n",
    "  # f.to_csv(\"../../full/data/filtered/srt_full_t_dpr.bed\", sep='\\t', index=False, header=False) # unsorted\n",
    "  # df2.to_csv(\"../data/deep_ATAC_STARR/filtered/srt_t_exch_dpr_p.bed\", sep='\\t', index=False, header=False) # unsorted\n",
    "  # df3.to_csv(\"../data/deep_ATAC_STARR/filtered/srt_t_exch_dpr_n.bed\", sep='\\t', index=False, header=False) # unsorted\n",
    "\n",
    "  # # print(f)\n",
    "  # # print(df2)\n",
    "  # cmds = [\"sort -k1,1 -V ../../full/data/PROcap_clear/filtered/srt_full_t_dpr.bed > ../../full/data/PROcap_clear/filtered/srt_full_dpr.bed\",\\\n",
    "  #       \"rm ../../full/data/PROcap_clear/filtered/srt_full_t_dpr.bed\",\\\n",
    "  #         \"sort -k1,1 -V ../data/PROcap_clear/filtered/srt_t_exch_dpr_p.bed > ../data/PROcap_clear/filtered/srt_exch_dpr_p.bed\",\\\n",
    "  #       \"rm ../data/PROcap_clear/filtered/srt_t_exch_dpr_p.bed\",\\\n",
    "  #         \"sort -k1,1 -V ../data/PROcap_clear/filtered/srt_t_exch_dpr_n.bed > ../data/PROcap_clear/filtered/srt_exch_dpr_n.bed\",\\\n",
    "  #       \"rm ../data/PROcap_clear/filtered/srt_t_exch_dpr_n.bed\"]\n",
    "  # for cmd in cmds:\n",
    "  #   os.system(cmd)\n",
    "  # print(f)\n",
    "  # raise Exception(\"Not implemented yet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STARR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
